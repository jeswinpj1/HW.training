from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from pymongo import MongoClient
import time

BASE_URL = "https://2xlhome.com/ae"

# ---------------- MongoDB ---------------- #
MONGO_URI = "mongodb://localhost:27017/"
DB_NAME = "scraper_db"
COLLECTION_CATEGORIES = "products"          # main categories + subcategories
COLLECTION_DETAILS = "product_details"      # detailed product info

client = MongoClient(MONGO_URI)
db = client[DB_NAME]
categories_col = db[COLLECTION_CATEGORIES]
details_col = db[COLLECTION_DETAILS]

# ---------------- Selenium ---------------- #
options = Options()
options.headless = False  # True if you want to run headless
driver = webdriver.Chrome(options=options)
driver.set_window_size(1200, 800)

def get_product_details(product_url):
    driver.get(product_url)
    time.sleep(2)  # wait for JS to render

    def safe_text(xpath):
        try:
            return driver.find_element(By.XPATH, xpath).text.strip()
        except:
            return ""

    title = safe_text('//h1[contains(@class,"page-title")]')
    price = safe_text('//span[contains(@class,"price new-aed-symbol")]')
    description = safe_text('//div[contains(@class,"collapsibleContent")]//div[contains(@class,"value")]/p')

    # Initialize all fields as empty
    material = ""
    color = ""
    height = ""
    width = ""
    length = ""

    # Extract specification fields
    try:
        specs = driver.find_elements(By.XPATH, '//div[contains(@class,"collapsibleContent")]//div[contains(@class,"value")]')
        for s in specs:
            try:
                key = s.find_element(By.XPATH, './strong').text.strip().lower()
                value = s.text.replace(s.find_element(By.XPATH, './strong').text, '').strip()
                if "material" in key:
                    material = value
                elif "color" in key:
                    color = value
                elif "height" in key:
                    height = value
                elif "width" in key:
                    width = value
                elif "length" in key:
                    length = value
            except:
                continue
    except:
        pass

    print(f"Scraped: {title}, {price}, {material}, {color}, {height}, {width}, {length}")
    return {
        "url": product_url,
        "title": title,
        "price": price,
        "description": description,
        "material": material,
        "color": color,
        "height": height,
        "width": width,
        "length": length
    }


def scrape_all_products():
    all_categories = list(categories_col.find())
    for cat in all_categories:
        for sub in cat.get("subcategories", []):
            sub_url = sub["url"]
            page_num = 1
            while True:
                paged_url = f"{sub_url}?p={page_num}"
                driver.get(paged_url)
                time.sleep(2)

                product_elements = driver.find_elements(By.XPATH, '//a[contains(@class,"product-item-link")]')
                if not product_elements:
                    break

                for i in range(len(product_elements)):
                    try:
                        prod_element = driver.find_elements(By.XPATH, '//a[contains(@class,"product-item-link")]')[i]
                        prod_url = prod_element.get_attribute("href")
                        details = get_product_details(prod_url)
                        details_col.insert_one(details)
                        print(f"Saved: {details['title']}")
                    except Exception as e:
                        print(f"Skipping product due to error: {e}")

                page_num += 1


if __name__ == "__main__":
    scrape_all_products()
    driver.quit()
    print("All product details scraped and saved to MongoDB.")
